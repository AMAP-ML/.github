# AMAP-ML

ü§ñ **Machine Learning @ Amap (AutoNavi), Alibaba Group**

We are the Machine Learning team at [Amap](https://amap.com/) (AutoNavi), focusing on delivering AI product and cutting-edge research in large language models, computer vision, generative AI, agent, world model, generative recommendation and intelligent mobility. Our work has been published at top-tier venues including **ICLR, AAAI, ICCV, EMNLP, ACM MM, and WWW**.

---

## üî• News

- **2026.02.06** üíª We open-sourced [![MobilityBench](https://img.shields.io/github/stars/AMAP-ML/MobilityBench?style=social&label=MobilityBench)](https://github.com/AMAP-ML/MobilityBench) -- A Scalable Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios.
- **2026.02.06** üéâ [![SpatialGenEval](https://img.shields.io/github/stars/AMAP-ML/SpatialGenEval?style=social&label=SpatialGenEval)](https://github.com/AMAP-ML/SpatialGenEval) is accepted by **ICLR 2026** -- Benchmarking Spatial Intelligence of Text-to-Image Models.
- **2026.02.06** üéâ [![Tree-GRPO](https://img.shields.io/github/stars/AMAP-ML/Tree-GRPO?style=social&label=Tree-GRPO)](https://github.com/AMAP-ML/Tree-GRPO) is accepted by **ICLR 2026** -- Tree Search for LLM Agent Reinforcement Learning.
- **2026.02.06** üéâ [![S2-Guidance](https://img.shields.io/github/stars/AMAP-ML/S2-Guidance?style=social&label=S2-Guidance)](https://github.com/AMAP-ML/S2-Guidance) is accepted by **ICLR 2026** -- Stochastic Self-Guidance for Training-Free Enhancement of Diffusion Models.
- **2026.02.05** üéâ [![MathForge](https://img.shields.io/github/stars/AMAP-ML/MathForge?style=social&label=MathForge)](https://github.com/AMAP-ML/MathForge) is accepted by **ICLR 2026** -- Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation.
- **2026.02.04** üíª We open-sourced [![Code2World](https://img.shields.io/github/stars/AMAP-ML/Code2World?style=social&label=Code2World)](https://github.com/AMAP-ML/Code2World) -- A GUI World Model via Renderable Code Generation.
- **2026.02.04** üéâ [![GPG](https://img.shields.io/github/stars/AMAP-ML/GPG?style=social&label=GPG)](https://github.com/AMAP-ML/GPG) is accepted by **ICLR 2026** -- A Simple and Strong Reinforcement Learning Baseline for Model Reasoning.
- **2026.02.04** üéâ [![NarrLV](https://img.shields.io/github/stars/AMAP-ML/NarrLV?style=social&label=NarrLV)](https://github.com/AMAP-ML/NarrLV) is accepted by **ICLR 2026** -- A Comprehensive Narrative-Centric Evaluation for Long Video Generation Models.
- **2026.02.04** üéâ [![EPG](https://img.shields.io/github/stars/AMAP-ML/EPG?style=social&label=EPG)](https://github.com/AMAP-ML/EPG) is accepted by **ICLR 2026** -- Advancing End-To-End Pixel-Space Generative Modeling via Self-Supervised Pre-Training.
- **2026.02.04** üéâ [![Omni-Effects](https://img.shields.io/github/stars/AMAP-ML/Omni-Effects?style=social&label=Omni-Effects)](https://github.com/AMAP-ML/Omni-Effects) is accepted by **AAAI 2026**.
- **2026.02.04** üéâ [![ImagerySearch](https://img.shields.io/github/stars/AMAP-ML/ImagerySearch?style=social&label=ImagerySearch)](https://github.com/AMAP-ML/ImagerySearch) is accepted by **AAAI 2026** -- Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints.
- **2026.02.04** üéâ [![Pos2Distill](https://img.shields.io/github/stars/AMAP-ML/Pos2Distill?style=social&label=Pos2Distill)](https://github.com/AMAP-ML/Pos2Distill) is accepted by **EMNLP 2025** -- Position Bias Mitigated via Inter-Position Knowledge Distillation.
- **2026.02.04** üéâ [![DSFNet](https://img.shields.io/github/stars/AMAP-ML/DSFNet?style=social&label=DSFNet)](https://github.com/AMAP-ML/DSFNet) is accepted by **WWW 2025** -- Disentangled Scenario Factorization for Multi-Scenario Route Ranking.
- **2026.02.04** üéâ [![UPRE](https://img.shields.io/github/stars/AMAP-ML/UPRE?style=social&label=UPRE)](https://github.com/AMAP-ML/UPRE) is accepted by **ICCV 2025** -- Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement.
- **2026.02.03** üéâ [![LD-RPS](https://img.shields.io/github/stars/AMAP-ML/LD-RPS?style=social&label=LD-RPS)](https://github.com/AMAP-ML/LD-RPS) is accepted by **ICCV 2025**.
- **2026.02.02** üíª We open-sourced [![AR-MAP](https://img.shields.io/github/stars/AMAP-ML/AR-MAP?style=social&label=AR-MAP)](https://github.com/AMAP-ML/AR-MAP).
- **2026.02.02** üéâ [![VMBench](https://img.shields.io/github/stars/AMAP-ML/VMBench?style=social&label=VMBench)](https://github.com/AMAP-ML/VMBench) is accepted by **ICCV 2025** -- A Benchmark for Perception-Aligned Video Motion Generation.
- **2026.01.31** üéâ [![SocioReasoner](https://img.shields.io/github/stars/AMAP-ML/SocioReasoner?style=social&label=SocioReasoner)](https://github.com/AMAP-ML/SocioReasoner) is accepted by **ICLR 2026** -- Urban Socio-Semantic Segmentation with Vision-Language Reasoning.
- **2026.01.29** üíª We open-sourced [![SpatialGenEval](https://img.shields.io/github/stars/AMAP-ML/SpatialGenEval?style=social&label=SpatialGenEval)](https://github.com/AMAP-ML/SpatialGenEval) -- Benchmarking Spatial Intelligence of Text-to-Image Models.
- **2026.01.28** üíª We open-sourced [![MathForge](https://img.shields.io/github/stars/AMAP-ML/MathForge?style=social&label=MathForge)](https://github.com/AMAP-ML/MathForge) -- Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation.
- **2026.01.23** üíª We open-sourced [![STV](https://img.shields.io/github/stars/AMAP-ML/STV?style=social&label=STV)](https://github.com/AMAP-ML/STV) -- Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning.
- **2026.01.21** üéâ [![USP](https://img.shields.io/github/stars/AMAP-ML/USP?style=social&label=USP)](https://github.com/AMAP-ML/USP) is accepted by **ICCV 2025** -- Unified Self-Supervised Pretraining for Image Generation and Understanding.
- **2026.01.15** üíª We open-sourced [![SocioReasoner](https://img.shields.io/github/stars/AMAP-ML/SocioReasoner?style=social&label=SocioReasoner)](https://github.com/AMAP-ML/SocioReasoner) -- Urban Socio-Semantic Segmentation with Vision-Language Reasoning.
- **2026.01.13** üéâ [![FingER](https://img.shields.io/github/stars/AMAP-ML/FingER?style=social&label=FingER)](https://github.com/AMAP-ML/FingER) is accepted by **ACM MM 2025** -- Content-Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos.
- **2026.01.13** üéâ [![HS-STaR](https://img.shields.io/github/stars/AMAP-ML/HS-STaR?style=social&label=HS-STaR)](https://github.com/AMAP-ML/HS-STaR) is accepted by **EMNLP 2025** -- Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation.
- **2026.01.13** üéâ [![SCALAR](https://img.shields.io/github/stars/AMAP-ML/SCALAR?style=social&label=SCALAR)](https://github.com/AMAP-ML/SCALAR) is accepted by **AAAI 2026** -- Scale-wise Controllable Visual Autoregressive Learning.
- **2026.01.07** üíª We open-sourced [![Thinking-with-Map](https://img.shields.io/github/stars/AMAP-ML/Thinking-with-Map?style=social&label=Thinking-with-Map)](https://github.com/AMAP-ML/Thinking-with-Map) -- Reinforced Parallel Map-Augmented Agent for Geolocalization.
- **2025.11.25** üíª We open-sourced [![SCALAR](https://img.shields.io/github/stars/AMAP-ML/SCALAR?style=social&label=SCALAR)](https://github.com/AMAP-ML/SCALAR) -- Scale-wise Controllable Visual Autoregressive Learning.
- **2025.11.18** üíª We open-sourced [![Eevee](https://img.shields.io/github/stars/AMAP-ML/Eevee?style=social&label=Eevee)](https://github.com/AMAP-ML/Eevee) -- Towards Close-up High-resolution Video-based Virtual Try-on.
- **2025.10.22** üíª We open-sourced [![Taming-Hallucinations](https://img.shields.io/github/stars/AMAP-ML/Taming-Hallucinations?style=social&label=Taming-Hallucinations)](https://github.com/AMAP-ML/Taming-Hallucinations).
- **2025.10.14** üíª We open-sourced [![EPG](https://img.shields.io/github/stars/AMAP-ML/EPG?style=social&label=EPG)](https://github.com/AMAP-ML/EPG) -- Advancing End-To-End Pixel-Space Generative Modeling via Self-Supervised Pre-Training.
- **2025.09.27** üíª We open-sourced [![HS-STaR](https://img.shields.io/github/stars/AMAP-ML/HS-STaR?style=social&label=HS-STaR)](https://github.com/AMAP-ML/HS-STaR) -- Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation.
- **2025.09.25** üíª We open-sourced [![Tree-GRPO](https://img.shields.io/github/stars/AMAP-ML/Tree-GRPO?style=social&label=Tree-GRPO)](https://github.com/AMAP-ML/Tree-GRPO) -- Tree Search for LLM Agent Reinforcement Learning.
- **2025.09.09** üíª We open-sourced [![DSFNet](https://img.shields.io/github/stars/AMAP-ML/DSFNet?style=social&label=DSFNet)](https://github.com/AMAP-ML/DSFNet) -- Disentangled Scenario Factorization for Multi-Scenario Route Ranking.
- **2025.08.29** üíª We open-sourced [![Pos2Distill](https://img.shields.io/github/stars/AMAP-ML/Pos2Distill?style=social&label=Pos2Distill)](https://github.com/AMAP-ML/Pos2Distill) -- Position Bias Mitigated via Inter-Position Knowledge Distillation.
- **2025.08.28** üíª We open-sourced [![FE2E](https://img.shields.io/github/stars/AMAP-ML/FE2E?style=social&label=FE2E)](https://github.com/AMAP-ML/FE2E).
- **2025.08.18** üíª We open-sourced [![ImagerySearch](https://img.shields.io/github/stars/AMAP-ML/ImagerySearch?style=social&label=ImagerySearch)](https://github.com/AMAP-ML/ImagerySearch) -- Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints.
- **2025.08.15** üíª We open-sourced [![S2-Guidance](https://img.shields.io/github/stars/AMAP-ML/S2-Guidance?style=social&label=S2-Guidance)](https://github.com/AMAP-ML/S2-Guidance) -- Stochastic Self-Guidance for Training-Free Enhancement of Diffusion Models.
- **2025.08.11** üíª We open-sourced [![Omni-Effects](https://img.shields.io/github/stars/AMAP-ML/Omni-Effects?style=social&label=Omni-Effects)](https://github.com/AMAP-ML/Omni-Effects).
- **2025.07.16** üíª We open-sourced [![UPRE](https://img.shields.io/github/stars/AMAP-ML/UPRE?style=social&label=UPRE)](https://github.com/AMAP-ML/UPRE) -- Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement.
- **2025.07.03** üíª We open-sourced [![LD-RPS](https://img.shields.io/github/stars/AMAP-ML/LD-RPS?style=social&label=LD-RPS)](https://github.com/AMAP-ML/LD-RPS).
- **2025.06.20** üíª We open-sourced [![FluxText](https://img.shields.io/github/stars/AMAP-ML/FluxText?style=social&label=FluxText)](https://github.com/AMAP-ML/FluxText) -- A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing.
- **2025.05.28** üíª We open-sourced [![FingER](https://img.shields.io/github/stars/AMAP-ML/FingER?style=social&label=FingER)](https://github.com/AMAP-ML/FingER) -- Content-Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos.
- **2025.05.21** üíª We open-sourced [![UniVG-R1](https://img.shields.io/github/stars/AMAP-ML/UniVG-R1?style=social&label=UniVG-R1)](https://github.com/AMAP-ML/UniVG-R1) -- Reasoning Guided Universal Visual Grounding with Reinforcement Learning.
- **2025.05.09** üíª We open-sourced [![NarrLV](https://img.shields.io/github/stars/AMAP-ML/NarrLV?style=social&label=NarrLV)](https://github.com/AMAP-ML/NarrLV) -- A Comprehensive Narrative-Centric Evaluation for Long Video Generation Models.
- **2025.04.07** üíª We open-sourced [![RealQA](https://img.shields.io/github/stars/AMAP-ML/RealQA?style=social&label=RealQA)](https://github.com/AMAP-ML/RealQA) -- Realistic Image Quality and Aesthetic Scoring with Multimodal LLM.
- **2025.04.03** üíª We open-sourced [![GPG](https://img.shields.io/github/stars/AMAP-ML/GPG?style=social&label=GPG)](https://github.com/AMAP-ML/GPG) -- A Simple and Strong Reinforcement Learning Baseline for Model Reasoning.
- **2025.03.12** üíª We open-sourced [![VMBench](https://img.shields.io/github/stars/AMAP-ML/VMBench?style=social&label=VMBench)](https://github.com/AMAP-ML/VMBench) -- A Benchmark for Perception-Aligned Video Motion Generation.
- **2025.03.11** üíª We open-sourced [![USP](https://img.shields.io/github/stars/AMAP-ML/USP?style=social&label=USP)](https://github.com/AMAP-ML/USP) -- Unified Self-Supervised Pretraining for Image Generation and Understanding.

---

## üìö Research Areas

### üß† LLM Reasoning & Reinforcement Learning

| Repository | Description | Venue |
|:--|:--|:--|
| [Tree-GRPO](https://github.com/AMAP-ML/Tree-GRPO) | Adopts tree-search rollouts in place of independent chain-based rollouts for LLM agent RL, achieving superior performance with only a quarter of the rollout budget. | ICLR 2026 |
| [GPG](https://github.com/AMAP-ML/GPG) | A minimalist RL approach (Group Policy Gradient) that directly optimizes the original RL objective, eliminating critic/reference models and KL constraints while outperforming GRPO. | ICLR 2026 |
| [MathForge](https://github.com/AMAP-ML/MathForge) | Proposes difficulty-aware GRPO and multi-aspect question reformulation to boost math reasoning by targeting harder questions from both algorithmic and data perspectives. | ICLR 2026 |
| [HS-STaR](https://github.com/AMAP-ML/HS-STaR) | A hierarchical sampling framework that identifies boundary-level problems and dynamically reallocates sampling budget toward high-utility problems for self-taught reasoners. | EMNLP 2025 |
| [Pos2Distill](https://github.com/AMAP-ML/Pos2Distill) | A position-to-position knowledge distillation framework that transfers knowledge from advantageous positions to mitigate position bias in LLMs. | EMNLP 2025 |

### üé® Image Generation & Editing

| Repository | Description | Venue |
|:--|:--|:--|
| [FluxText](https://github.com/AMAP-ML/FluxText) | A novel text editing framework for multi-line scene text in complex visual scenarios, with Condition Injection LoRA module and regional text perceptual loss. | - |
| [S2-Guidance](https://github.com/AMAP-ML/S2-Guidance) | Leverages stochastic block-dropping to construct sub-networks for training-free guidance, surpassing CFG on text-to-image and text-to-video generation. | ICLR 2026 |
| [EPG](https://github.com/AMAP-ML/EPG) | Advancing end-to-end pixel-space generative modeling via self-supervised pre-training. | ICLR 2026 |
| [Omni-Effects](https://github.com/AMAP-ML/Omni-Effects) | A unified framework for prompt-guided and spatially controllable composite visual effects generation, using LoRA-MoE and spatial-aware prompts. | AAAI 2026 |
| [SCALAR](https://github.com/AMAP-ML/SCALAR) | Scale-wise controllable visual autoregressive learning for image generation. | AAAI 2026 |
| [USP](https://github.com/AMAP-ML/USP) | Unified self-supervised pretraining via masked latent modeling in VAE space, significantly improving diffusion model convergence and generation quality. | ICCV 2025 |
| [SpatialGenEval](https://github.com/AMAP-ML/SpatialGenEval) | A benchmark with 1,230 information-dense prompts and 12,300 multi-choice questions to evaluate complex spatial intelligence in text-to-image models. | ICLR 2026 |

### üé¨ Video Generation & Understanding

| Repository | Description | Venue |
|:--|:--|:--|
| [NarrLV](https://github.com/AMAP-ML/NarrLV) | The first benchmark to comprehensively evaluate narrative expression capabilities of long video generation models, inspired by film narrative theory. | ICLR 2026 |
| [ImagerySearch](https://github.com/AMAP-ML/ImagerySearch) | A prompt-guided adaptive test-time search strategy that dynamically adjusts search space and reward for imaginative video generation with long-distance semantic dependencies. | AAAI 2026 |
| [VMBench](https://github.com/AMAP-ML/VMBench) | A perception-aligned video motion benchmark with human-aligned metrics achieving 35.3% improvement in Spearman's correlation over baselines. | ICCV 2025 |
| [Eevee](https://github.com/AMAP-ML/Eevee) | A high-resolution dataset and benchmark for video-based virtual try-on, supporting both full-shot and close-up garment detail views. | - |
| [FingER](https://github.com/AMAP-ML/FingER) | Content-aware fine-grained evaluation with reasoning for AI-generated videos. | ACM MM 2025 |
| [FE2E](https://github.com/AMAP-ML/FE2E) | - | - |

### üëÅÔ∏è Multimodal & Vision-Language Models

| Repository | Description | Venue |
|:--|:--|:--|
| [UniVG-R1](https://github.com/AMAP-ML/UniVG-R1) | Reasoning guided universal visual grounding with reinforcement learning. | - |
| [SocioReasoner](https://github.com/AMAP-ML/SocioReasoner) | A vision-language reasoning framework for urban socio-semantic segmentation that simulates human annotation via cross-modal recognition and multi-stage RL-based reasoning. | ICLR 2026 |
| [RealQA](https://github.com/AMAP-ML/RealQA) | A 14,715-image UGC dataset with 10 fine-grained attributes for realistic image quality and aesthetic scoring; achieves SOTA on 5 public IQA/IAA benchmarks using next-token prediction. | - |
| [Code2World](https://github.com/AMAP-ML/Code2World) | A GUI world model via renderable code generation. | - |
| [Taming-Hallucinations](https://github.com/AMAP-ML/Taming-Hallucinations) | - | - |
| [STV](https://github.com/AMAP-ML/STV) | A sensitivity-aware task vector insertion framework that identifies context-sensitive heads and selects task vectors via RL for many-shot multimodal in-context learning. | AAAI 2026 |

### üó∫Ô∏è Maps, Mobility & Spatial Intelligence

| Repository | Description | Venue |
|:--|:--|:--|
| [Thinking-with-Map](https://github.com/AMAP-ML/Thinking-with-Map) | A map-augmented agent that conducts reasoning with real-world maps for geolocalization, trained via reinforcement learning. | - |
| [MobilityBench](https://github.com/AMAP-ML/MobilityBench) | A scalable benchmark for evaluating route-planning agents in real-world mobility scenarios. | - |
| [DSFNet](https://github.com/AMAP-ML/DSFNet) | Disentangled scenario factorization for multi-scenario route ranking with the first large-scale public MSDR dataset; deployed in AMap for online traffic. | WWW 2025 |
| [AR-MAP](https://github.com/AMAP-ML/AR-MAP) | - | - |

### üîç Object Detection & Segmentation

| Repository | Description | Venue |
|:--|:--|:--|
| [UPRE](https://github.com/AMAP-ML/UPRE) | Zero-shot domain adaptation for object detection via unified prompt and representation enhancement. | CCV 2025 |
| [LD-RPS](https://github.com/AMAP-ML/LD-RPS) | LD-RPS. | ICCV 2025 |
